<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriately as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="See, Say, and Segment: Teaching LMMs to Overcome False Premises">
    <meta property="og:title" content="SESAME" />
    <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
    <meta property="og:url" content="http://see-say-segment.github.io" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X630-->
    <meta property="og:image" content="/static/images/sesame_banner.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />


    <meta name="twitter:title" content="See, Say, and Segment: Teaching LMMs to Overcome False Premises">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
    <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X600-->
    <meta name="twitter:image" content="static/images/sesame_twitter_banner.png">
    <meta name="twitter:card" content="SESAME Project Logo: A cartoon bun with sesame seeds on top.">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords"
        content="Large Multimodal Models, segmentation, reasoning, VQA, referring expression segmentation">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>See, Say and Segment: Teaching LMMs to Overcome False Premises</title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon_io/favicon.ico">
    <link rel="apple-touch-icon" sizes="180x180" href="static/images/favicon_io">
    <link rel="icon" type="image/png" sizes="32x32" href="static/images/favicon_io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="static/images/favicon_io/favicon-16x16.png">
    <link rel="manifest" href="static/images/favicon_io/site.webmanifest">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/fontawesome/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/fontawesome/js/fontawesome.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>

<body>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered ">
                        <h1 class="title is-1 publication-title">See, Say, and Segment: Teaching LMMs to Overcome False
                            Premises</h1>
                    </div>
                </div>
                <div class="container is-max-desktop">
                    <div class="columns is-centered">
                        <div class="column has-text-centered is-four-fifths">
                            <div class="is-size-5 publication-authors">
                                <!-- Paper authors -->
                                <span class="author-block">
                                    <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Tsung-Han
                                        Wu</a><sup>*</sup>,</span>
                                <span class="author-block">
                                    <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Giscard
                                        Biamby</a><sup>*</sup>,</span>
                                <span class="author-block">
                                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">David Chan</a>
                                </span>
                                <span class="author-block">
                                    <a href="FOURTH AUTHOR PERSONAL LINK" target="_blank">Lisa Dunlap</a>
                                </span>
                                <span class="author-block">
                                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Ritwik Gupta</a>
                                </span>
                                <span class="author-block">
                                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Xudong Wang</a>
                                </span>
                                <span class="author-block">
                                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Joseph E. Gonzalez</a>
                                </span>
                                <span class="author-block">
                                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Trevor Darrell</a>
                                </span>
                            </div>

                            <div class="is-size-5 publication-authors">
                                <span class="author-block">UC Berkeley<br><span
                                        style="display:none;">Conference</span></span>
                                <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                            </div>

                            <div class="column has-text-centered">
                                <div class="publication-links">
                                    <!-- Arxiv PDF link -->
                                    <span class="link-block">
                                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fas fa-file-pdf"></i>
                                            </span>
                                            <span>Paper</span>
                                        </a>
                                    </span>

                                    <!-- Supplementary PDF link -->
                                    <span class="link-block">
                                        <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fas fa-file-pdf"></i>
                                            </span>
                                            <span>Supplementary</span>
                                        </a>
                                    </span>

                                    <!-- Github link -->
                                    <span class="link-block">
                                        <a href="https://github.com/see-say-segment/sesame" target="_blank"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fa-brands fa-github"></i>
                                            </span>
                                            <span>Code</span>
                                        </a>
                                    </span>

                                    <!-- ArXiv abstract Link -->
                                    <span class="link-block">
                                        <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="ai ai-arxiv"></i>
                                            </span>
                                            <span>arXiv</span>
                                        </a>
                                    </span>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <!-- Paper abstract -->
    <section class="section hero is-light">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Current open-source Large Multimodal Models (LMMs) excel at tasks such as open-vocabulary
                            language grounding and segmentation but can suffer under false premises, when queries imply
                            the existence of something that is not actually present in the image.

                            We observe that existing methods that fine-tune an LMM to segment images significantly
                            degrade their ability to reliably determine ("see") if an object is present, a form of
                            catastrophic forgetting. In this work, we propose cascading and/or joint training LMMs to
                            solve this task, avoiding catastrophic forgetting of previous skills. Our resulting model
                            can "see" by detecting whether objects are present in an image, "say" by telling the
                            user if they are not, proposing alternative queries or correcting semantic errors in the
                            query, and finally``segment'' by outputting the mask of the desired objects if they exist.
                            We introduce a novel dataset and benchmark of false premise queries for existing
                            RefCOCO(+/g) data (which we call FP-RefCOCO(+/g)), and demonstrate that our method not
                            only detects false premises up to 55% better than existing approaches, but under false
                            premise conditions produces relative cIOU improvements of more than 31% over baselines, and
                            produces natural language feedback judged helpful up to 67% of the time.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!-- End paper abstract -->



    <!-- SESAME -->
    <section class="section  is-light">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full">
                    <h2 class="title is-2">See, Say and Segment</h2>
                </div>
            </div>
            <div id="results-carousel" class="carousel results-carousel">
                <div class="item">
                    <img src="static/images/figures/fig1_SSS.png"
                        alt="Figure 1: False Premise Referring Expression Segmentation Examples with prior work and our method." />
                    <h2 class="subtitle has-text-centered">
                        Contemporary open-source LMMs combined with segmentation decoders
                        are able to generate referring segmentations but have difficulty on expressions
                        which refer to something that is not present in the image. <strong>SESAME</strong>, our
                        See-Say-Segment LMM, uses joint training to overcome this problem.
                    </h2>
                </div>
                <div class="item is-vcentered">
                    <img src="static/images/figures/fig2_SSS.png" alt="Figure 2: Diagram of SESAME Model Framework" />
                    <h2 class="subtitle has-text-centered">
                        We introduce <strong>a Novel Problem Setting</strong>, requiring LMMs that can See, Say and
                        Segment. To facilitate training and evaluation of this new class of models, we release <strong>a
                            new dataset and benchmark, FP-RefCOCO, FP-RefCOCO+ and FP-RefCOCOg</strong>.
                    </h2>
                </div>
            </div>
        </div>
    </section>



    <!-- DATASET -->
    <section class="section hero is-light">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-2">FP-RefCOCO: A Novel Dataset</h2>
                    <div class="content has-text-justified">
                        <p>
                            To facilitate training and evaluation of this new class of models, we release <strong>a new
                                dataset and benchmark, FP-RefCOCO, FP-RefCOCO+ and FP-RefCOCOg</strong>.
                        </p>
                        <!-- <img class="img" src="static/images/figures/fig3_SSS.png"> -->
                    </div>
                </div>
            </div>

            <div id="results-carousel" class="carousel results-carousel block">
                <div class="item">
                    <img src="static/images/figures/data_1.png" alt="MY ALT TEXT" />
                    <h2 class="subtitle has-text-centered">
                        &nbsp;<br />
                        Using refCOCO for base images, we employ an LLM to create a false-premise referring
                        segmentation dataset with similar objects, attributes, and relations. Such paired examples
                        enable the the creation of specific correction ground truth that is more specific than baseline
                        methods which simply sample positive and negative examples. This data allows us to train an
                        LMM that has robust reasoning reference capabilities.
                    </h2>
                </div>
                <div class="item is-vcentered">
                    <img src="static/images/figures/data_2.png" alt="MY ALT TEXT" />
                    <h2 class="subtitle has-text-centered">
                        Existing datasets such as R-RefCOCO include queries referring to non-existent items in images, but their method of generating negative expressions through naive random sampling often lacks context-awareness. This limitation significantly reduces their effectiveness for false-premise correction tasks. <br/><br/>
                        
                        Consider an image with a cat on a chair: contextually valid false premises that could be logically corrected to "a cat on the chair" might include phrases like "a cat under the chair" or "a dog on the chair." However, R-RefCOCO typically produces less suitable examples, such as ``a pizza on the chair” or "a cat behind the people," which do not align with realistic model correction expectations. 
                    </h2>
                </div>
            </div>

    </section>


    <section class="section hero is-light">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-2">Examples</h2>

                    <img class="img" src="static/images/figures/fig4_cherrypick.png">
                    <img class="img" src="static/images/figures/supp_cherrypick.png">
                    <img class="img" src="static/images/figures/supp_say.png">
                </div>
            </div>
        </div>
    </section>








    <!-- Paper poster -->
    <!-- <section class="hero is-small is-light">
        <div class="hero-body">
            <div class="container">
                <h2 class="title">Poster</h2>

                <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
                </iframe>

            </div>
        </div>
    </section> -->
    <!--End paper poster -->


    <!--BibTex citation -->
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>BibTex Code Here</code></pre>
        </div>
    </section>
    <!--End BibTex citation -->


    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">

                        <p>
                            This page was built using the <a
                                href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
                            You are free to borrow the of this website, we just ask that you link back to this page in
                            the footer. <br> This website is licensed under a <a rel="license"
                                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                                Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>

                    </div>
                </div>
            </div>
        </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

</body>

</html>